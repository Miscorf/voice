{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 基本库\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 加载音频处理库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 其他库\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import glob "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 特征提取以及数据集的建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aloe': 0, 'burger': 1, 'cabbage': 2, 'candied_fruits': 3, 'carrots': 4, 'chips': 5, 'chocolate': 6, 'drinks': 7, 'fries': 8, 'grapes': 9, 'gummies': 10, 'ice-cream': 11, 'jelly': 12, 'noodles': 13, 'pickles': 14, 'pizza': 15, 'ribs': 16, 'salmon': 17, 'soup': 18, 'wings': 19}\n",
      "{0: 'aloe', 1: 'burger', 2: 'cabbage', 3: 'candied_fruits', 4: 'carrots', 5: 'chips', 6: 'chocolate', 7: 'drinks', 8: 'fries', 9: 'grapes', 10: 'gummies', 11: 'ice-cream', 12: 'jelly', 13: 'noodles', 14: 'pickles', 15: 'pizza', 16: 'ribs', 17: 'salmon', 18: 'soup', 19: 'wings'}\n"
     ]
    }
   ],
   "source": [
    "feature = []\n",
    "label = []\n",
    "# 建立类别标签，不同类别对应不同的数字。\n",
    "label_dict = {'aloe': 0, 'burger': 1, 'cabbage': 2,'candied_fruits':3, 'carrots': 4, 'chips':5,\n",
    "                  'chocolate': 6, 'drinks': 7, 'fries': 8, 'grapes': 9, 'gummies': 10, 'ice-cream':11,\n",
    "                  'jelly': 12, 'noodles': 13, 'pickles': 14, 'pizza': 15, 'ribs': 16, 'salmon':17,\n",
    "                  'soup': 18, 'wings': 19}\n",
    "label_dict_inv = {v:k for k,v in label_dict.items()}\n",
    "print(label_dict)\n",
    "print(label_dict_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def extract_features(parent_dir, sub_dirs, max_file=10, file_ext=\"*.wav\"):\n",
    "    c = 0\n",
    "    label, feature = [], []\n",
    "    for sub_dir in sub_dirs:\n",
    "        for fn in tqdm(glob.glob(os.path.join(parent_dir, sub_dir, file_ext))[:max_file]): # 遍历数据集的所有文件\n",
    "            \n",
    "            # segment_log_specgrams, segment_labels = [], []\n",
    "            #sound_clip,sr = librosa.load(fn)\n",
    "            # print(fn)\n",
    "            label_name = fn.split('/')[-2]\n",
    "            # print(label_name)\n",
    "            \n",
    "            label.extend([label_dict[label_name]])\n",
    "            X, sample_rate = librosa.load(fn,res_type='kaiser_fast')\n",
    "            mels = np.mean(librosa.feature.melspectrogram(y=X,sr=sample_rate).T,axis=0) # 计算梅尔频谱(mel spectrogram),并把它作为特征\n",
    "            feature.extend([mels])\n",
    "    \n",
    "    return [feature, label]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:04<00:00,  9.04it/s]\n",
      "100%|██████████| 64/64 [00:07<00:00,  8.72it/s]\n",
      "100%|██████████| 48/48 [00:08<00:00,  5.74it/s]\n",
      "100%|██████████| 74/74 [00:12<00:00,  6.05it/s]\n",
      "100%|██████████| 49/49 [00:07<00:00,  6.59it/s]\n",
      "100%|██████████| 57/57 [00:08<00:00,  6.36it/s]\n",
      "100%|██████████| 27/27 [00:04<00:00,  6.71it/s]\n",
      "100%|██████████| 27/27 [00:03<00:00,  7.10it/s]\n",
      "100%|██████████| 57/57 [00:08<00:00,  7.09it/s]\n",
      "100%|██████████| 61/61 [00:08<00:00,  6.82it/s]\n",
      "100%|██████████| 65/65 [00:10<00:00,  6.31it/s]\n",
      "100%|██████████| 69/69 [00:10<00:00,  6.39it/s]\n",
      "100%|██████████| 43/43 [00:06<00:00,  6.76it/s]\n",
      "100%|██████████| 33/33 [00:04<00:00,  6.99it/s]\n",
      "100%|██████████| 75/75 [00:11<00:00,  6.29it/s]\n",
      "100%|██████████| 55/55 [00:09<00:00,  5.82it/s]\n",
      "100%|██████████| 47/47 [00:07<00:00,  6.22it/s]\n",
      "100%|██████████| 37/37 [00:06<00:00,  5.88it/s]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.97it/s]\n",
      "100%|██████████| 35/35 [00:05<00:00,  6.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# 自己更改目录  \n",
    "parent_dir = 'data/train_sample/'\n",
    "save_dir = \"data\"\n",
    "folds = sub_dirs = np.array(['aloe','burger','cabbage','candied_fruits',\n",
    "                             'carrots','chips','chocolate','drinks','fries',\n",
    "                            'grapes','gummies','ice-cream','jelly','noodles','pickles',\n",
    "                            'pizza','ribs','salmon','soup','wings'])\n",
    "\n",
    "# 获取特征feature以及类别的label\n",
    "\n",
    "temp = extract_features(parent_dir,sub_dirs,max_file=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250264/1712219130.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  temp = np.array(temp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X的特征尺寸是 (1000, 128)\n",
      "Y的特征尺寸是 (1000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 15:06:28.852557: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/anaconda3/lib:/cm/shared/apps/hdf5/1.10.1/lib:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda10.2/toolkit/10.2.89/targets/x86_64-linux/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64:/cm/shared/apps/gcc8/8.4.0/lib:/cm/shared/apps/gcc8/8.4.0/lib32:/cm/shared/apps/gcc8/8.4.0/lib64\n",
      "2022-09-07 15:06:28.852599: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "temp = np.array(temp)\n",
    "data = temp.transpose()\n",
    "# 转置\n",
    "# 获取特征\n",
    "X = np.vstack(data[:, 0])\n",
    "\n",
    "# 获取标签\n",
    "Y = np.array(data[:, 1],dtype= int)\n",
    "print('X的特征尺寸是',X.shape)\n",
    "print('Y的特征尺寸是',Y.shape)\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "# Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 128)\n",
      "(1000, 1, 1, 16, 8)\n",
      "(1000, 1)\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# 最终数据\n",
    "print(X.shape)\n",
    "# X = X.reshape(-1, 1, 16, 8, 1)\n",
    "X = X.reshape(-1,1, 1, 16, 8)\n",
    "Y = Y.reshape(-1,1)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Y.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "# from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 1, 16, 8])\n",
      "tensor([13])\n"
     ]
    }
   ],
   "source": [
    "b = torch.from_numpy(X)\n",
    "#c = Y\n",
    "c = torch.from_numpy(Y)\n",
    "print(b.size())\n",
    "X_train, X_test, y_train, y_test = train_test_split(b, c, test_size=0.1, random_state=42)\n",
    "print(y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于分布式光纤传感器的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN2D(nn.module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(1,6,3)\n",
    "#         self.conv2 = nn.Conv2d(1,6,3)\n",
    "#         self.bn1 = nn.BatchNorm1d()\n",
    "#         self.maxpool1 = nn.MaxPool2d(2,2)\n",
    "#         self.conv3 = nn.Conv2d(6,25,5)\n",
    "#         self.conv4 = nn.Conv2d(6,25,5)\n",
    "#         #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,64,3,padding=2)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(64,128,3,padding=2)\n",
    "        # self.conv3 = nn.Conv2d(128,16,3,padding=2)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "        self.fla = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(240*8,1024)\n",
    "        self.fc2 = nn.Linear(1024,20)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.pool(x)\n",
    "        # tanh 较好\n",
    "        x = self.conv2(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.3,training=self.training)\n",
    "        # x = x.view(-1, (28*28)//(4*4)*8)\n",
    "        x = x.view(-1,240*8)\n",
    "        x = self.fc1(x)\n",
    "        x = F.tanh(x)\n",
    "\n",
    "        x = F.dropout(x, p=0.2,training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        # x = sfmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4402)\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "predicted = torch.tensor([[1,2,3,4]]).float()\n",
    "target = torch.tensor([1]).long()\n",
    "lossfxn = nn.CrossEntropyLoss() # 已经集成一部分softmax，无需重复使用\n",
    "loss = lossfxn(predicted, target)\n",
    "print(loss) # outputs tensor(2.4402)\n",
    "print(predicted)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 3, 32, 32])\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (residule_layer0): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (residule_layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (residule_layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (residule_layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "torch.Size([50, 10])\n"
     ]
    }
   ],
   "source": [
    "from demo import resnet\n",
    "resnet.test_output_shape()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 1, 16, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiabaojun/.conda/envs/foodVoice/lib/python3.9/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2.6278, grad_fn=<NllLossBackward>)\n",
      "1 tensor(2.4256, grad_fn=<NllLossBackward>)\n",
      "2 tensor(2.0221, grad_fn=<NllLossBackward>)\n",
      "3 tensor(1.8190, grad_fn=<NllLossBackward>)\n",
      "4 tensor(1.4389, grad_fn=<NllLossBackward>)\n",
      "5 tensor(1.6667, grad_fn=<NllLossBackward>)\n",
      "6 tensor(1.2203, grad_fn=<NllLossBackward>)\n",
      "7 tensor(1.2899, grad_fn=<NllLossBackward>)\n",
      "8 tensor(0.6060, grad_fn=<NllLossBackward>)\n",
      "9 tensor(0.6018, grad_fn=<NllLossBackward>)\n",
      "10 tensor(0.1397, grad_fn=<NllLossBackward>)\n",
      "11 tensor(0.1724, grad_fn=<NllLossBackward>)\n",
      "12 tensor(0.0887, grad_fn=<NllLossBackward>)\n",
      "13 tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "14 tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "15 tensor(0.0604, grad_fn=<NllLossBackward>)\n",
      "16 tensor(0.0496, grad_fn=<NllLossBackward>)\n",
      "17 tensor(0.0046, grad_fn=<NllLossBackward>)\n",
      "18 tensor(0.0040, grad_fn=<NllLossBackward>)\n",
      "19 tensor(0.0002, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = model()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(net.parameters(),lr = 0.001,momentum=0.9)\n",
    "print(b.shape)\n",
    "for epoch in range(20):\n",
    "    for i,da in enumerate(X_train):\n",
    "        x = da\n",
    "        #print(x)\n",
    "        y = y_train[i]\n",
    "\n",
    "        # print(y)\n",
    "        net.train() #打开dropout\n",
    "        pred = net(x)\n",
    "        #print(pred)\n",
    "        loss = loss_fn(pred,y)\n",
    "        # print(i,loss)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print(epoch,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(a,b):\n",
    "    acc  = 0\n",
    "    for i,da in enumerate(a):\n",
    "        x = da\n",
    "        # print(x)\n",
    "        y = b[i]\n",
    "        pred = net(x)\n",
    "        pred = pred\n",
    "        # print(pred)\n",
    "        m = max(pred[0])\n",
    "        # print(m)\n",
    "        pre_list = pred[0].tolist()\n",
    "        ind = pre_list.index(m)\n",
    "        \n",
    "        y_list = y.tolist()\n",
    "        #print(y_list)\n",
    "        # y_max = max(y_list)\n",
    "        # y_ind = y_list.index(y_max)\n",
    "\n",
    "        if ind == y_list[0]:\n",
    "            acc += 1\n",
    "        # print(ind, y_ind)\n",
    "    print(acc/len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36\n",
      "0.7522222222222222\n"
     ]
    }
   ],
   "source": [
    "accuracy(X_test,y_test)\n",
    "accuracy(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('foodVoice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [
    {
     "id": "96728",
     "title": "获取数据集标题失败"
    }
   ],
   "description": "",
   "notebookId": "185525",
   "source": "dsw"
  },
  "vscode": {
   "interpreter": {
    "hash": "a35a5ff58b060d3e8e40d5a274dfca8c253152deb5bb2796b761379e6aa5cd6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
